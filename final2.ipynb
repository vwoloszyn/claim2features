{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim2Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Hot Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def myHotEncode(input_data, max_vocab=0, vocab2idx=None):\n",
    "    \"Return the hot-vecotor and the vocab2idx.\"\n",
    "    if vocab2idx is None:\n",
    "        vocabFreq = {}\n",
    "        for i in input_data:\n",
    "            for j in i:\n",
    "                if j not in vocabFreq:\n",
    "                    vocabFreq[j] = 0\n",
    "                vocabFreq[j] += 1\n",
    "        vocabFreq = OrderedDict(sorted(vocabFreq.items(), key=itemgetter(1), reverse=True))\n",
    "        vocab2idx = {}\n",
    "        count = 0\n",
    "        for v in vocabFreq:\n",
    "            count += 1\n",
    "            if max_vocab > 0 and count > max_vocab:\n",
    "                break\n",
    "            vocab2idx[v] = len(vocab2idx)        \n",
    "    vocabEmbeddings = np.identity(len(vocab2idx), dtype='float32')\n",
    "    data_ret = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        for j in i:\n",
    "            if j in vocab2idx:\n",
    "                i_.append(vocabEmbeddings[vocab2idx[j]])\n",
    "        if len(i_) == 0:\n",
    "            i_ = np.zeros((1,len(vocab2idx)))\n",
    "        data_ret.append(sum(i_))\n",
    "    return data_ret, vocab2idx\n",
    "\n",
    "\n",
    "def myHotDecode(input_data, vocab2idx):\n",
    "    \"Return the decode as final representation and decode as indexs\"\n",
    "    data_ = []\n",
    "    data_idx = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        i_idx = []\n",
    "        if len(i) != len(vocab2idx):\n",
    "            print('Erro:', 'The vocab2idx not fit the input data!')\n",
    "            return\n",
    "        for _i, j in enumerate(i):\n",
    "            if j > 0:\n",
    "                v = [k for k in vocab2idx if vocab2idx[k]==_i][0]\n",
    "                i_.append(v)\n",
    "                i_idx.append(_i)\n",
    "        data_.append(i_)\n",
    "        data_idx.append(i_idx)\n",
    "    return data_, data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treat text or a list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_text(text, removes=['\\n', '\\r', '\\t'], strip=True):\n",
    "    if type(text) == list:\n",
    "        text_ = []\n",
    "        for t in text:\n",
    "            text_.append(treat_text(t))\n",
    "        return text_\n",
    "    text_ = text\n",
    "    for remove in removes:\n",
    "        text_ = text_.replace(remove, '')\n",
    "    if strip:\n",
    "        text_ = text_.strip()\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split pure words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_words(text, dividers=['-', '_'], uppercase=True, low_upper=True):\n",
    "    upper = list('ABCDEFGHIJKLMNOPQRSTUVWYZ')\n",
    "    text_ = ''\n",
    "    last_space = True\n",
    "    for c in text:\n",
    "        if not last_space:\n",
    "            if c in dividers:\n",
    "                text_ += ' '\n",
    "            elif uppercase and c in upper:\n",
    "                c_ = c\n",
    "                if low_upper:\n",
    "                    c_ = c.lower()\n",
    "                text_ += ' ' + c_\n",
    "            else:\n",
    "                text_ += c\n",
    "        else:\n",
    "            if c not in dividers:\n",
    "                text_ += c\n",
    "        if c == ' ':\n",
    "            last_space = True\n",
    "        else:\n",
    "            last_space = False\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "data_path = 'dataset.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "columns = ['tag', 'text', 'attrs', 'level', 'brother_tag', 'brother_text', 'brother_attrs', 'url', 'site', 'label']\n",
    "words_context = []\n",
    "brother_words_context = []\n",
    "\n",
    "count = 0\n",
    "for tupla in df[columns].values:\n",
    "    count += 1\n",
    "    # Each instance is a dictionary dic\n",
    "    dic = {}\n",
    "    for i, column in enumerate(columns):\n",
    "        dic[column] = tupla[i]\n",
    "    \n",
    "    # words_context attribute\n",
    "    words_context_ = ''\n",
    "    if dic['text'] != 'None Text':\n",
    "        words_context_ += pure_words(treat_text(str(dic['text']))) + ' '\n",
    "    attrs = ast.literal_eval(dic['attrs'])\n",
    "    for attr in attrs.values():\n",
    "        if type(attr) == str:\n",
    "            words_context_ += pure_words(treat_text(str(attr))) + ' '\n",
    "        else:\n",
    "            for a in attr:\n",
    "                words_context_ += pure_words(treat_text(str(a))) + ' '\n",
    "    words_context.append(words_context_)\n",
    "    \n",
    "    # brother_words_context attribute\n",
    "    brother_words_context_ = ''\n",
    "    if dic['brother_text'] != 'None Text':\n",
    "        brother_words_context_ += pure_words(treat_text(str(dic['brother_text']))) + ' '\n",
    "    attrs = ast.literal_eval(dic['brother_attrs'])\n",
    "    for attr in attrs.values():\n",
    "        if type(attr) == str:\n",
    "            brother_words_context_ += pure_words(treat_text(str(attr))) + ' '\n",
    "        else:\n",
    "            for a in attr:\n",
    "                brother_words_context_ += pure_words(treat_text(str(a))) + ' '\n",
    "    brother_words_context.append(brother_words_context_)\n",
    "    \n",
    "del df['attrs']\n",
    "del df['brother_attrs']\n",
    "df['words_context'] = words_context\n",
    "df['brother_words_context'] = brother_words_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words for context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "vocab_size = 2000\n",
    "vec = CountVectorizer(max_features=vocab_size, stop_words=ENGLISH_STOP_WORDS)\n",
    "vec.fit(df['words_context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
