{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_html_tags = ['script']\n",
    "file_output = 'dataset.csv'\n",
    "\n",
    "MAX_URLS_SITE = 10\n",
    "MIN_TEXT_LEVEL = 5\n",
    "\n",
    "def get_soup(url, timeout=10):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "    try:\n",
    "        request = urllib.request.urlopen(urllib.request.Request(url, data=None, headers={'User-Agent': user_agent}), timeout=timeout)\n",
    "    except:\n",
    "        return 'timeout'\n",
    "    request.status\n",
    "    page = request.read()\n",
    "    return BeautifulSoup(page, 'lxml')\n",
    "\n",
    "def treat_text(text):\n",
    "    if type(text) == list:\n",
    "        text_ = []\n",
    "        for t in text:\n",
    "            text_.append(t.replace('\\n', '').replace('\\t', '').replace('\\r','').strip())\n",
    "        return text_\n",
    "    return text.replace('\\n', '').replace('\\t', '').replace('\\r','').strip()\n",
    "\n",
    "def validate_text(text, level):\n",
    "    if level >= MIN_TEXT_LEVEL:\n",
    "        return treat_text(text)[:2000]\n",
    "    return 'None Text'\n",
    "\n",
    "def validate_text_dict(dictionary):\n",
    "    for k in dictionary:\n",
    "        dictionary[k] = treat_text(dictionary[k])\n",
    "    return dictionary\n",
    "\n",
    "def validate_attrs(attrs):\n",
    "    new_attrs = []\n",
    "    for d in attrs:\n",
    "        new_attrs.append(validate_text_dict(d))\n",
    "    return new_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snopes():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 100\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "        url_ = \"https://www.snopes.com/fact-check/page/\" + str(page_number) + \"/\"\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll('a', {\"class\": \"article-link\"}, href=True)\n",
    "        for anchor in links:\n",
    "            url = anchor['href']\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            snopes_walk_html(soup, data_, 'snope', url, 0, dic_=dic_)\n",
    "            if dic_['claim'] and dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "            \n",
    "def snopes_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or (features['tag'] == 'h1' and\n",
    "         'class' in features['attrs'] and\n",
    "         'article-title' in features['attrs']['class'])):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif ((features['tag'] == 'meta' and\n",
    "          'itemprop' in features['attrs'] and\n",
    "          'datePublished' in features['attrs']['itemprop']) or\n",
    "          (features['tag'] == 'span' and\n",
    "          'class' in features['attrs'] and\n",
    "          'date-wrapper' in features['attrs']['class']) or\n",
    "          (features['tag'] == 'span' and\n",
    "          'itemprop' in features['attrs'] and\n",
    "          'itemReviewed' in features['itemprop'])):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Claim\n",
    "    elif (features['tag'] == 'p' and \n",
    "          len(brothers) > 0 and\n",
    "          'class' in brothers[-1]['attrs'] and\n",
    "          'claim' in brothers[-1]['attrs']['class'] and\n",
    "          'section-break' in brothers[-1]['attrs']['class']):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "    \n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'a' and \n",
    "          parent is not None and\n",
    "          parent['tag'] == 'div' and\n",
    "          'class' in parent['attrs'] and\n",
    "          'rating-wrapper' in parent['attrs']['class']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(snopes_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullfact():\n",
    "    alfab = \"bcdefghijklmnopqrstuvxyz\"\n",
    "    #alfab = \"b\"\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 20\n",
    "    count = 0\n",
    "    count_url = 0\n",
    "    for l in alfab:\n",
    "        for page_number in range(1, pages+1):\n",
    "            print('')\n",
    "            count += 1\n",
    "            print(count, '/', (pages)*len(alfab), '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "            url_ = \"http://fullfact.org/search/?q=\" + l + \"&page=\" + str(page_number)\n",
    "            try:\n",
    "                soup_ = get_soup(url_)\n",
    "                if type(soup_) == str:\n",
    "                    print('timeout', end='')\n",
    "                    continue\n",
    "            except:\n",
    "                print(\"Error urlopen.\", url_)\n",
    "                continue\n",
    "            soup_.prettify('utf-8')\n",
    "            links = soup_.findAll('a', {\"rel\": \"bookmark\"}, href=True)\n",
    "            for anchor in links:\n",
    "                url = \"http://fullfact.org\" + anchor['href']\n",
    "                if url in urls__:\n",
    "                    continue\n",
    "                urls__.append(url)\n",
    "                soup = get_soup(url, timeout=5)\n",
    "                if type(soup) == type(''):\n",
    "                    print('timeout', end='')\n",
    "                    continue\n",
    "                count_url += 1\n",
    "                if count_url > MAX_URLS_SITE:\n",
    "                    return data\n",
    "                soup.prettify(\"utf-8\")\n",
    "                data_ = []\n",
    "                dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "                fullfact_walk_html(soup, data_, 'fullfact', url, 0, dic_=dic_)\n",
    "                if dic_['claim'] and dic_['credibility']:\n",
    "                    print('.', end='')\n",
    "                    data += data_\n",
    "                else:\n",
    "                    print(',', end='')\n",
    "                    count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "\n",
    "def fullfact_walk_html(element, data, site, url, level, parent=None, brothers=None, dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or (features['tag'] == 'h1' and\n",
    "         'class' not in features['attrs'])):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif (features['tag'] == 'p' and\n",
    "          'class' in features['attrs'] and\n",
    "          'date' in features['attrs']['class']):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Claim\n",
    "    elif (features['tag'] == 'p' and \n",
    "          len(brothers) > 0 and\n",
    "          'Claim' == brothers[-1]['text']):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "    \n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'p' and \n",
    "          len(brothers) > 0 and\n",
    "          'Conclusion' == brothers[-1]['text']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(fullfact_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politifact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politifact():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 100\n",
    "    count = 0\n",
    "    types=[\"true\",\"mostly-true\",\"half-true\",\"barely-true\",\"false\",\"pants-fire\",\"no-flip\",\"half-flip\",\"full-flop\"]\n",
    "    count_url = 0\n",
    "    for type_ in types:\n",
    "        for page_number in range (1, pages+1):\n",
    "            count += 1\n",
    "            print('')\n",
    "            print(count, '/', (pages)*len(types), '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "            url_ = \"http://www.politifact.com/truth-o-meter/rulings/\" + str(type_) + \"/?page=\" + str(page_number)\n",
    "            try:\n",
    "                soup_ = get_soup(url_)\n",
    "            except:\n",
    "                print(\"Error urlopen.\", url_)\n",
    "                continue\n",
    "            soup_.prettify('utf-8')\n",
    "            links = soup_.findAll(\"p\", {\"class\": \"statement__text\"})\n",
    "            for anchor in links:\n",
    "                anchor = anchor.find('a', {\"class\": \"link\"}, href=True)\n",
    "                url = \"http://www.politifact.com\" + str(anchor['href'])\n",
    "                if url in urls__:\n",
    "                    continue\n",
    "                urls__.append(url)\n",
    "                soup = get_soup(url, timeout=5)\n",
    "                if type(soup) == type(''):\n",
    "                    print('timeout', end='')\n",
    "                    continue\n",
    "                count_url += 1\n",
    "                if count_url > MAX_URLS_SITE:\n",
    "                    return data\n",
    "                soup.prettify(\"utf-8\")\n",
    "                data_ = []\n",
    "                dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "                politifact_walk_html(soup, data_, 'politifact', url, 0, dic_=dic_)\n",
    "                if dic_['claim']:\n",
    "                    print('.', end='')\n",
    "                    data += data_\n",
    "                else:\n",
    "                    print(',', end='')\n",
    "                    count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "\n",
    "def politifact_walk_html(element, data, site, url, level, parent=None, brothers=None, dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or (features['tag'] == 'h1' and\n",
    "         'class' in features['attrs'] and\n",
    "         'article__title' in features['attrs']['class'])):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif ((features['tag'] == 'p' and\n",
    "          'class' in parent['attrs'] and\n",
    "          'widget__content-xs' in parent['attrs']['class'] and\n",
    "          'Published' in features['text']) or\n",
    "          (features['tag'] == 'p' and\n",
    "          'class' in parent['attrs'] and\n",
    "          'widget__content' in parent['attrs']['class'] and\n",
    "          'Published' in features['text'])):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Claim\n",
    "    elif ((features['tag'] == 'div' and \n",
    "          'class' in features['attrs'] and\n",
    "          'statement__text' in features['attrs']['class']) or\n",
    "          (features['tag'] == 'div' and \n",
    "          'class' in features['attrs'] and\n",
    "          'sharethefacts-statement' in features['attrs']['class'])):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "    \n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'img' and \n",
    "          'class' in features['attrs'] and\n",
    "          'statement-detail' in features['attrs']['class']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(politifact_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruthorFiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truthorfiction():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 100\n",
    "    count = 0\n",
    "    types=[\"a\"]\n",
    "    count_url = 0\n",
    "    for type_ in types:\n",
    "        for page_number in range (1, pages+1):\n",
    "            count += 1\n",
    "            print('')\n",
    "            print(count, '/', (pages)*len(types), '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "            url_ = \"https://www.truthorfiction.com/page/\" + str(page_number) + \"/?s=\" + str(type_)\n",
    "            try:\n",
    "                soup_ = get_soup(url_)\n",
    "            except:\n",
    "                print(\"Error urlopen.\", url_)\n",
    "                continue\n",
    "            soup_.prettify('utf-8')\n",
    "            links = soup_.findAll(\"h2\", {\"class\": \"grid-title\"})\n",
    "            for anchor in links:\n",
    "                anchor = anchor.find('a', href=True)\n",
    "                url = str(anchor['href'])\n",
    "                if url in urls__:\n",
    "                    continue\n",
    "                urls__.append(url)\n",
    "                soup = get_soup(url, timeout=5)\n",
    "                if type(soup) == type(''):\n",
    "                    print('timeout', end='')\n",
    "                    continue\n",
    "                count_url += 1\n",
    "                if count_url > MAX_URLS_SITE:\n",
    "                    return data\n",
    "                soup.prettify(\"utf-8\")\n",
    "                data_ = []\n",
    "                dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "                truthorfiction_walk_html(soup, data_, 'truthorfiction', url, 0, dic_=dic_)\n",
    "                if dic_['claim'] and dic_['credibility']:\n",
    "                    print('.', end='')\n",
    "                    data += data_\n",
    "                else:\n",
    "                    print(',', end='')\n",
    "                    count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "\n",
    "def truthorfiction_walk_html(element, data, site, url, level, parent=None, brothers=None, dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or features['tag'] == 'h1'):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif ((features['tag'] == 'span' and\n",
    "           parent['tag'] == 'div' and\n",
    "          'class' in parent['attrs'] and\n",
    "          'post-box-meta-single' in parent['attrs']['class']) or\n",
    "          (features['tag'] == 'div' and\n",
    "           'class' in features['attrs'] and\n",
    "           'post-box-meta-single' in features['attrs']['class'])):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Claim\n",
    "    elif (features['tag'] == 'p' and\n",
    "          len(brothers) > 0 and \n",
    "          'Summary of eRumor' in brothers[-1]['text']):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "    \n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'p' and\n",
    "          len(brothers) > 0 and \n",
    "          'The Truth' in brothers[-1]['text']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(truthorfiction_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Africa Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def africacheck():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 30\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "        url_ = 'https://africacheck.org/latest-reports/page/' + str(page_number)\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll('div', {\"class\": \"article-content\"})\n",
    "        for anchor in links:\n",
    "            url = anchor.find('a', href=True)['href']\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            print('.', end='')\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            africacheck_walk_html(soup, data_, 'africacheck', url, 0, dic_=dic_)\n",
    "            if dic_['claim'] and dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "            \n",
    "def africacheck_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or (features['tag'] == 'h1')):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif (features['tag'] == 'time' and 'Published' in features['text']):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Claim\n",
    "    elif ('class' in features['attrs'] and \n",
    "          ('claim-content' in features['attrs']['class'] or \n",
    "           'the-content' in features['attrs']['class'] or\n",
    "           'report-claim' in features['attrs']['class'])):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "    \n",
    "    # Credibility\n",
    "    elif ('class' in features['attrs'] and \n",
    "          ('report-verdict' in features['attrs']['class'] or\n",
    "           'indicator' in features['attrs']['class'])):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(africacheck_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheckYourFact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkyourfact():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 20\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "        \n",
    "        url_ = 'http://checkyourfact.com/page/' + str(page_number)\n",
    "        \n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.find('articles').findAll('a', href=True)\n",
    "        for anchor in links:\n",
    "            url = 'http://checkyourfact.com' + anchor['href']\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            checkyourfact_walk_html(soup, data_, 'checkyourfact', url, 0, dic_=dic_)\n",
    "            if dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "            \n",
    "def checkyourfact_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or \n",
    "        (features['tag'] == 'h1' and \n",
    "         parent['tag'] == 'article')):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif ((features['tag'] == 'time' and\n",
    "          parent['tag'] == 'article')):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Credibility \n",
    "    elif (features['tag'] == 'p' and\n",
    "          'Verdict: ' in features['text']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(checkyourfact_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TheFerret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theferret():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 9\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "        url_ = 'https://theferret.scot/category/fact-check/page/' + str(page_number)\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll('article')\n",
    "        for anchor in links:\n",
    "            url = anchor.find('a', href=True)['href'] \n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            theferret_walk_html(soup, data_, 'theferret', url, 0, dic_=dic_)\n",
    "            if dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "            \n",
    "def theferret_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or \n",
    "        (features['tag'] == 'h1' and\n",
    "        'class' in features['attrs'] and\n",
    "        ('cover-title' in features['attrs']['class'] or\n",
    "         'title' in features['attrs']['class']))):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif (features['tag'] == 'time'):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'h3' and\n",
    "          'Ferret Fact Service verdict:' in features['text']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(theferret_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theconversation():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 10\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "        \n",
    "        url_ = \"https://theconversation.com/us/topics/factcheck-6544?page=\" + str(page_number)\n",
    "        \n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll('a', {'class': 'article-link'}, href=True)\n",
    "        for anchor in links:\n",
    "            url = anchor['href']\n",
    "            url = \"https://theconversation.com\" + url\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            theconversation_walk_html(soup, data_, 'theconversation', url, 0, dic_=dic_)\n",
    "            if dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "            \n",
    "def theconversation_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "    \n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or \n",
    "        (features['tag'] == 'h1' and\n",
    "         'class' in features['attrs'])):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "    \n",
    "    # Date\n",
    "    elif (features['tag'] == 'time'):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "    \n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'p' and\n",
    "          len(brothers) > 0 and\n",
    "          brothers[-1]['tag'] == 'h2' and\n",
    "          brothers[-1]['text'] == 'Verdict'):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "    \n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "    \n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(theconversation_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Washingtonpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def washingtonpost():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 100 \n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "\n",
    "        url_ = \"https://www.washingtonpost.com/news/fact-checker/page/\" + str(page_number)\n",
    "\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll(\"div\",{\"class\":\"story-headline\"})\n",
    "        for anchor in links:\n",
    "            url = str(anchor.find(\"a\", href=True)['href'])\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            washingtonpost_walk_html(soup, data_, 'washingtonpost', url, 0, dic_=dic_)\n",
    "            if True:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "def washingtonpost_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "\n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or (features['tag'] == 'h1')):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "\n",
    "    # Date\n",
    "    elif (features['tag'] == 'span' and\n",
    "          'class' in features['attrs'] and\n",
    "          'author-timestamp' in features['attrs']['class']):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "\n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "\n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(washingtonpost_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rappler():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 16\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "\n",
    "        url_ = \"https://www.rappler.com/newsbreak/fact-check?start=\" + str(page_number*10 - 10)\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll('h4')\n",
    "        for anchor in links:\n",
    "            url = 'https://www.rappler.com' + anchor.find('a', href=True)['href']\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            rappler_walk_html(soup, data_, 'rappler', url, 0, dic_=dic_)\n",
    "            if dic_['claim'] and dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "def rappler_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "\n",
    "    # Title\n",
    "    if (features['tag'] == 'title' \n",
    "        or (features['tag'] == 'h1')):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "\n",
    "    # Date\n",
    "    elif (features['tag'] == 'div' and\n",
    "          'class' in features['attrs'] and\n",
    "          'published' in features['attrs']['class']):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "\n",
    "    # Claim\n",
    "    elif (features['tag'] == 'p' and \n",
    "          'Claim:' in features['text']):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "\n",
    "    # Credibility\n",
    "    elif ('Rating:' in features['text']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "\n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "\n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(rappler_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verafiles():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 1\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "\n",
    "        url_ = \"http://verafiles.org/rundown?ccm_paging_p=\" + str(page_number)\n",
    "\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll('div', {'class': 'page-list-article__title'})\n",
    "        for anchor in links:\n",
    "            url = anchor.find('a', href=True)['href']\n",
    "            print(url)\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            verafiles_walk_html(soup, data_, 'verafiles', url, 0, dic_=dic_)\n",
    "            if dic_['claim'] and dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "def verafiles_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "\n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or (features['tag'] == 'h1')):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "\n",
    "    # Date\n",
    "    elif (features['tag'] == 'p' and\n",
    "          len(brothers) > 0 and\n",
    "          brothers[-1]['tag'] == 'p' and\n",
    "          'DATE' in brothers[-1]['text']):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "\n",
    "    # Claim\n",
    "    elif (parent and parent['tag'] == 'main'):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "        print(features['tag'] , '**********')\n",
    "\n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'div' and\n",
    "          'class' in features['attrs'] and\n",
    "          'sharethefacts-rating' in features['attrs']['class']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "\n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "\n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(verafiles_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factcheckni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factcheckni():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 11\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "\n",
    "        url_ = \"https://factcheckni.org/page/\" + str(page_number)\n",
    "\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.findAll('article')\n",
    "        for anchor in links:\n",
    "            url = anchor.find('a', href=True)['href'] \n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            factcheckni_walk_html(soup, data_, 'factcheckni', url, 0, dic_=dic_)\n",
    "            if dic_['claim'] and dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "def factcheckni_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "\n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or \n",
    "        features['tag'] == 'h1'):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "\n",
    "    # Date\n",
    "    elif (features['tag'] == 'span' and\n",
    "          'class' in features['attrs'] and\n",
    "          'posted-on' in features['attrs']['class']):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "\n",
    "    # Claim\n",
    "    elif (features['tag'] == 'p' and\n",
    "          'CLAIM: ' in features['text']):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "\n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'p' and\n",
    "          'CONCLUSION: ' in features['text']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "\n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "\n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(factcheckni_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_)) # EDIT - SITENAME\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc():\n",
    "    data = []\n",
    "    urls__ = []\n",
    "    pages = 10\n",
    "    count_url = 0\n",
    "    for page_number in range(1, pages+1):\n",
    "        print('')\n",
    "        print(page_number, '/', pages, '  [', count_url, '/', MAX_URLS_SITE, ']', end='')\n",
    "\n",
    "        url_ = \"https://www.abc.net.au/news/factcheck/factchecks/?page=\" + str(page_number)\n",
    "\n",
    "        try:\n",
    "            soup_ = get_soup(url_)\n",
    "            if type(soup_) == str:\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "        except:\n",
    "            print(\"Error urlopen.\", url_)\n",
    "            continue\n",
    "        soup_.prettify('utf-8')\n",
    "        links = soup_.find(\"ul\", {\"class\":\"article-index\"}).findAll('a', href=True)\n",
    "        for anchor in links:\n",
    "            url = \"https://www.abc.net.au\" + anchor['href']\n",
    "            if url in urls__:\n",
    "                continue\n",
    "            urls__.append(url)\n",
    "            soup = get_soup(url, timeout=5)\n",
    "            if type(soup) == type(''):\n",
    "                print('timeout', end='')\n",
    "                continue\n",
    "            count_url += 1\n",
    "            if count_url > MAX_URLS_SITE:\n",
    "                return data\n",
    "            soup.prettify(\"utf-8\")\n",
    "            data_ = []\n",
    "            dic_ = {'claim': 0, 'credibility': 0, 'date': 0, 'title': 0}\n",
    "            abc_walk_html(soup, data_, 'abc', url, 0, dic_=dic_)\n",
    "            if dic_['claim'] and dic_['credibility']:\n",
    "                print('.', end='')\n",
    "                data += data_\n",
    "            else:\n",
    "                print(',', end='')\n",
    "                count_url -= 1\n",
    "    if count_url < MAX_URLS_SITE:\n",
    "        print('Warning:', count_url, 'URLS was include')\n",
    "    return data\n",
    "\n",
    "def abc_walk_html(element, data, site, url, level, parent=None, brothers=[], dic_=None):\n",
    "    features = {}\n",
    "    if element is None:\n",
    "        return\n",
    "    features['site'] = site\n",
    "    features['url'] = url\n",
    "    features['tag'] = element.name\n",
    "    features['attrs'] = validate_text_dict(element.attrs)\n",
    "    features['level'] = level\n",
    "    features['text'] = validate_text(element.text, level)\n",
    "\n",
    "    if brothers:\n",
    "        features['brother_tag'] = brothers[-1]['tag']\n",
    "        features['brother_attrs'] = brothers[-1]['attrs']\n",
    "        features['brother_text'] = brothers[-1]['text']\n",
    "    else:\n",
    "        features['brother_tag'] = ''\n",
    "        features['brother_attrs'] = {}\n",
    "        features['brother_text'] = 'NONE'\n",
    "\n",
    "    # Title\n",
    "    if (features['tag'] == 'title' or features['tag'] == 'h1'):\n",
    "        features['label'] = 'Title'\n",
    "        dic_['title'] += 1\n",
    "\n",
    "    # Date\n",
    "    elif (features['tag'] == 'time'):\n",
    "        features['label'] = 'Date'\n",
    "        dic_['date'] += 1\n",
    "\n",
    "    # Claim\n",
    "    elif (features['tag'] == 'p' and\n",
    "          brothers and\n",
    "          brothers[-1]['tag'] == 'h2' and\n",
    "          'The claim' in brothers[-1]['text']):\n",
    "        features['label'] = 'Claim'\n",
    "        dic_['claim'] += 1\n",
    "\n",
    "    # Credibility\n",
    "    elif (features['tag'] == 'p' and\n",
    "          brothers and\n",
    "          brothers[-1]['tag'] == 'h2' and\n",
    "          'The verdict' in brothers[-1]['text']):\n",
    "        features['label'] = 'Credibility'\n",
    "        dic_['credibility'] += 1\n",
    "\n",
    "    # None\n",
    "    else:\n",
    "        features['label'] = 'None'\n",
    "\n",
    "    data.append(features)\n",
    "    brothers_ = []\n",
    "    for e_child in element:\n",
    "        if (type(e_child) == bs4.element.NavigableString or\n",
    "            type(e_child) == bs4.element.Comment or\n",
    "            type(e_child) == bs4.element.Doctype or\n",
    "            type(e_child) == str or e_child.name in ignore_html_tags):\n",
    "                continue\n",
    "        brothers_.append(abc_walk_html(e_child, data, site, url, level+1, parent=features, brothers=brothers_, dic_=dic_))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 100   [ 0 / 10 ].........."
     ]
    }
   ],
   "source": [
    "data_ = snopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FullFact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 480   [ 0 / 10 ],,,,,,,,,,,,,,,,,,,,\n",
      "2 / 480   [ 0 / 10 ],,,.,,,,,,,,,,,,,,,,\n",
      "3 / 480   [ 1 / 10 ],,,,,.,,,,,,,,,,,,,,\n",
      "4 / 480   [ 2 / 10 ],,,.,,,,,,,,,,,,,.,,\n",
      "5 / 480   [ 4 / 10 ].,,,,,,,,,,.,,,,,,,.\n",
      "6 / 480   [ 7 / 10 ],.,,,,.,,,,,,,,,,,,,\n",
      "7 / 480   [ 9 / 10 ],,."
     ]
    }
   ],
   "source": [
    "data_ = fullfact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PolitiFact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Doesn't have conclusion</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 900   [ 0 / 10 ].........."
     ]
    }
   ],
   "source": [
    "data_ = politifact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TruthorFiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 100   [ 0 / 10 ]....................\n",
      "2 / 100   [ 10 / 10 ]"
     ]
    }
   ],
   "source": [
    "data_ = truthorfiction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AfricaCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 30   [ 0 / 10 ]....................\n",
      "2 / 30   [ 10 / 10 ]"
     ]
    }
   ],
   "source": [
    "data_ = africacheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckYourFact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Doesn't have claim</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 20   [ 0 / 10 ]..,..,......"
     ]
    }
   ],
   "source": [
    "data_ = checkyourfact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TheFerret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Doesn't have claim</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 9   [ 0 / 10 ]..timeouttimeout.....\n",
      "2 / 9   [ 7 / 10 ].,,timeouttimeout.timeouttimeout,\n",
      "3 / 9   [ 9 / 10 ]timeouttimeouttimeouttimeout,timeouttimeouttimeouttimeout\n",
      "4 / 9   [ 9 / 10 ]timeout,timeout.timeout"
     ]
    }
   ],
   "source": [
    "data_ = theferret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TheConversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Doesn't have claim</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 10   [ 0 / 10 ],..,.......,,."
     ]
    }
   ],
   "source": [
    "data_ = theconversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Washingtonpost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>The code is not complete</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_ = washingtonpost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rappler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 16   [ 0 / 10 ].,........\n",
      "2 / 16   [ 9 / 10 ]."
     ]
    }
   ],
   "source": [
    "data_ = rappler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verafiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Current, the extractor is not working for claim and credibility</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_ = verafiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factcheckni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 11   [ 0 / 10 ].....,\n",
      "2 / 11   [ 5 / 10 ]...,,.\n",
      "3 / 11   [ 9 / 10 ]."
     ]
    }
   ],
   "source": [
    "data_ = factcheckni()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 10   [ 0 / 10 ]timeout\n",
      "2 / 10   [ 0 / 10 ],,,,,,,,,,,,.,,,,,,,,,,.,timeout,,,.,.,.,,,,.,.,,.,,,.,,,,."
     ]
    }
   ],
   "source": [
    "data_ = abc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv(file_output, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
