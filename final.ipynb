{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def myHotEncode(input_data, max_vocab=0, vocab2idx=None):\n",
    "    \"Return the hot-vecotor and the vocab2idx.\"\n",
    "    if vocab2idx is None:\n",
    "        vocabFreq = {}\n",
    "        for i in input_data:\n",
    "            for j in i:\n",
    "                if j not in vocabFreq:\n",
    "                    vocabFreq[j] = 0\n",
    "                vocabFreq[j] += 1\n",
    "        vocabFreq = OrderedDict(sorted(vocabFreq.items(), key=itemgetter(1), reverse=True))\n",
    "        vocab2idx = {}\n",
    "        count = 0\n",
    "        for v in vocabFreq:\n",
    "            count += 1\n",
    "            if max_vocab > 0 and count > max_vocab:\n",
    "                break\n",
    "            vocab2idx[v] = len(vocab2idx)        \n",
    "    vocabEmbeddings = np.identity(len(vocab2idx), dtype='float32')\n",
    "    data_ret = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        for j in i:\n",
    "            if j in vocab2idx:\n",
    "                i_.append(vocabEmbeddings[vocab2idx[j]])\n",
    "        data_ret.append(sum(i_))\n",
    "    if len(data_ret) == 0:\n",
    "        data_ret = np.zeros(len(vocab2idx))\n",
    "    return data_ret, vocab2idx\n",
    "\n",
    "\n",
    "def myHotDecode(input_data, vocab2idx):\n",
    "    \"Return the decode as final representation and decode as indexs\"\n",
    "    data_ = []\n",
    "    data_idx = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        i_idx = []\n",
    "        if len(i) != len(vocab2idx):\n",
    "            print('Erro:', 'The vocab2idx not fit the input data!')\n",
    "            return\n",
    "        for _i, j in enumerate(i):\n",
    "            if j > 0:\n",
    "                v = [k for k in vocab2idx if vocab2idx[k]==_i][0]\n",
    "                i_.append(v)\n",
    "                i_idx.append(_i)\n",
    "        data_.append(i_)\n",
    "        data_idx.append(i_idx)\n",
    "    return data_, data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treat a list of str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def treat_str_list(input_list):\n",
    "    ret = []\n",
    "    for s in input_list:\n",
    "        ret += [ss.lower().strip() for ss in re.split('[\\W_]+', s)]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_path = 'features_annotated.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "columns = ['attrs', 'label', 'level', 'tag', 'text', 'url']\n",
    "\n",
    "last_url = ''\n",
    "#batches = []\n",
    "\n",
    "css_tss = []\n",
    "id_tss = []\n",
    "level_tss = []\n",
    "tag_tss = []\n",
    "url_tss = []\n",
    "css_ts = []\n",
    "id_ts = []\n",
    "level_ts = []\n",
    "tag_ts = []\n",
    "url_ts = []\n",
    "\n",
    "labels = []\n",
    "#labels_batch = []\n",
    "label = 'None'\n",
    "\n",
    "last_level = -1\n",
    "\n",
    "for attr, label_, level, tag, text, url in df[columns].values:\n",
    "    attr = ast.literal_eval(attr)\n",
    "    if url != last_url:\n",
    "        css_tss.append(css_ts.copy())\n",
    "        id_tss.append(id_ts.copy())\n",
    "        level_tss.append(level_ts.copy())\n",
    "        tag_tss.append(tag_ts.copy())\n",
    "        css_ts = []\n",
    "        id_ts = []\n",
    "        level_ts = []\n",
    "        tag_ts = []\n",
    "        #labels_batch.append(label)\n",
    "        #aux_list = []\n",
    "        #aux_list.append(css_tss.copy())\n",
    "        #aux_list.append(id_tss.copy())\n",
    "        #aux_list.append(level_tss.copy())\n",
    "        #aux_list.append(tag_tss.copy())\n",
    "        #batches.append(aux_list)\n",
    "        #css_tss = []\n",
    "        #id_tss = []\n",
    "        #level_tss = []\n",
    "        #tag_tss = []\n",
    "        labels.append(label) # new\n",
    "        #labels.append(labels_batch.copy())\n",
    "        #labels_batch = []\n",
    "        last_url = url\n",
    "        label = 'None'\n",
    "    if not level > last_level:\n",
    "        css_tss.append(css_ts.copy())\n",
    "        id_tss.append(id_ts.copy())\n",
    "        level_tss.append(level_ts.copy())\n",
    "        tag_tss.append(tag_ts.copy())\n",
    "        css_ts = css_ts[:level]\n",
    "        id_ts = id_ts[:level]\n",
    "        level_ts = level_ts[:level]\n",
    "        tag_ts = tag_ts[:level]\n",
    "        #labels_batch.append(label)\n",
    "        labels.append(label) # new\n",
    "        label = 'None'\n",
    "    if label_ != 'None':\n",
    "        label = label_\n",
    "    # Features\n",
    "    if 'class' in attr:\n",
    "        css_ts.append(treat_str_list(attr['class']))\n",
    "        #css_ts += treat_str_list(attr['class'])\n",
    "    else:\n",
    "        css_ts.append([])\n",
    "    aux_list = []\n",
    "    if 'id' in attr:\n",
    "        #aux_list.append(treat_str_list([attr['id']]))\n",
    "        aux_list += treat_str_list([attr['id']])\n",
    "    if 'name' in attr:\n",
    "        #aux_list.append(treat_str_list([attr['name']]))\n",
    "        aux_list += treat_str_list([attr['name']])\n",
    "        \n",
    "    id_ts.append(aux_list)\n",
    "    tag_ts.append([tag])\n",
    "    level_ts.append([level])\n",
    "    last_level = level\n",
    "\n",
    "css_tss.append(css_ts.copy())\n",
    "id_tss.append(id_ts.copy())\n",
    "level_tss.append(level_ts.copy())\n",
    "tag_tss.append(tag_ts.copy())\n",
    "#labels_batch.append(label)\n",
    "#aux_list = []\n",
    "#aux_list.append(css_tss.copy())\n",
    "#aux_list.append(id_tss.copy())\n",
    "#aux_list.append(level_tss.copy())\n",
    "#aux_list.append(tag_tss.copy())\n",
    "#batches.append(aux_list)\n",
    "#labels.append(labels_batch.copy())\n",
    "labels.append(label) # new\n",
    "#batches = batches[1:]\n",
    "css_tss = css_tss[1:] # new\n",
    "id_tss = id_tss[1:] # new\n",
    "level_tss = level_tss[1:] # new\n",
    "tag_tss = tag_tss[1:] # new\n",
    "url_tss = url_tss[1:] # new\n",
    "labels = labels[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_ = []\n",
    "id_ = []\n",
    "tag_ = []\n",
    "for css in css_tss:\n",
    "    for i in css:\n",
    "        css_.append(i)\n",
    "for _id in id_tss:\n",
    "    for i in _id:\n",
    "        id_.append(i)\n",
    "for tag in tag_tss:\n",
    "    for i in tag:\n",
    "        tag_.append(i)\n",
    "_, css2Idx = myHotEncode(css_, 100)\n",
    "_, id2Idx = myHotEncode(id_, 100)\n",
    "_, tag2Idx = myHotEncode(tag_, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, label2Idx = myHotEncode([[l] for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i0, batch in enumerate(batches):\n",
    "# CSS classes\n",
    "for i1, sample in enumerate(css_tss):\n",
    "    for i2, timestep in enumerate(sample):\n",
    "        timestep_, _ = myHotEncode(timestep, vocab2idx=css2Idx)\n",
    "        css_tss[i1][i2] = timestep_\n",
    "# ID and Names\n",
    "for i1, sample in enumerate(id_tss):\n",
    "    for i2, timestep in enumerate(sample):\n",
    "        timestep_, _ = myHotEncode(timestep, vocab2idx=id2Idx)\n",
    "        id_tss[i1][i2] = timestep_\n",
    "# Tags\n",
    "for i1, sample in enumerate(tag_tss):\n",
    "    for i2, timestep in enumerate(sample):\n",
    "        timestep_, _ = myHotEncode(timestep, vocab2idx=tag2Idx)\n",
    "        tag_tss[i1][i2] = timestep_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate,Concatenate\n",
    "\n",
    "\n",
    "def lstm_model():\n",
    "    input_css = Input(shape=(None, len(css2Idx)), name='input_css')\n",
    "    css = Dense(len(css2Idx))(input_css)\n",
    "    \n",
    "    input_id = Input(shape=(None, len(id2Idx)), name='input_id')\n",
    "    _id = Dense(len(id2Idx))(input_id)\n",
    "    \n",
    "    input_level = Input(shape=(None, 1), name='input_level')\n",
    "    level = Dense(1)(input_level)\n",
    "    \n",
    "    input_tag = Input(shape=(None, len(tag2Idx)), name='input_tag')\n",
    "    tag = Dense(len(tag2Idx))(input_tag)\n",
    "    \n",
    "    output = concatenate([css, _id, tag])\n",
    "    output = LSTM(256)(output)\n",
    "    output = Dense(len(label2Idx), activation='softmax')\n",
    "    model = Model(inputs=[input_css, input_id, input_level, input_tag], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "confusion = []\n",
    "\n",
    "gp = GroupKFold(n_splits=5)\n",
    "for train_indexs, test_indexs in gp.split(features['tags'], groups=features['groups']):\n",
    "    \n",
    "    #_trains = {}\n",
    "    #for i in train_indexs:\n",
    "    #    group_ = features['groups'][i]\n",
    "    #    tag_ = features['tags'][i]\n",
    "    #    level_ = features['levels'][i]\n",
    "    #    label_ = features['labels'][i]\n",
    "    #    if group_ not in _trains:\n",
    "    #        _trains[group_] = [[], [], []]\n",
    "    #    _trains[group_][0].append(tag_)\n",
    "    #    _trains[group_][1].append(level_)\n",
    "    #    _trains[group_][2].append(label_)\n",
    "\n",
    "    #X_train = [[np.array(_trains[x][0]), np.array(_trains[x][1])] for x in _trains]\n",
    "    #y_train = [np.array(_trains[y][-1]) for y in _trains]\n",
    "    \n",
    "    \n",
    "    tags_train = features['tags'][train_indexs]\n",
    "    tags_train = tags_train.reshape((tags_train.shape[0], 1, tags_train.shape[1])) # Test\n",
    "    levels_train = features['levels'][train_indexs]\n",
    "    levels_train = levels_train.reshape((levels_train.shape[0], 1, levels_train.shape[1])) # Test\n",
    "    X_train = [tags_train, levels_train]\n",
    "    y_train = features['labels'][train_indexs]\n",
    "    \n",
    "    tags_test = features['tags'][test_indexs]\n",
    "    tags_test = tags_test.reshape((tags_test.shape[0], 1, tags_test.shape[1])) # Test\n",
    "    levels_test = features['levels'][test_indexs]\n",
    "    levels_test = levels_test.reshape((levels_test.shape[0], 1, levels_test.shape[1])) # Test\n",
    "    X_test = [tags_test, levels_test]\n",
    "    y_test = features['labels'][test_indexs]\n",
    "\n",
    "    groups_train = features['groups'][train_indexs]\n",
    "    groups_test = features['groups'][test_indexs]\n",
    "    train_g = set()\n",
    "    test_g = set()\n",
    "    for g in groups_train:\n",
    "        train_g.add(g)\n",
    "    for g in groups_test:\n",
    "        test_g.add(g)\n",
    "    print('Train Groups (URLs)', train_g)\n",
    "    print('Test Groups (URLs)', test_g)\n",
    "    \n",
    "    #model = KerasClassifier(build_fn=lstm_model)\n",
    "    model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "    #model.fit(tags_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "    result = model.predict(X_test)\n",
    "    #result = model.predict(tags_test)\n",
    "    y_test = [r.tolist().index(1) for r in y_test]\n",
    "    acc = accuracy_score(result, y_test)\n",
    "    accuracy.append(acc)\n",
    "    p = precision_score(result, y_test, average=\"macro\")\n",
    "    precision.append(p)\n",
    "    r = recall_score(result, y_test, average=\"macro\")\n",
    "    recall.append(r)\n",
    "    f = f1_score(result, y_test, average=\"macro\")\n",
    "    f1.append(f)\n",
    "    confusion.append(confusion_matrix(result, y_test))\n",
    "    \n",
    "    print(\"%s: %.2f %%\" % ('Acc', acc*100))\n",
    "    print(\"%s: %.2f %%\" % ('Precision', p*100))\n",
    "    print(\"%s: %.2f %%\" % ('Recall', r*100))\n",
    "    print(\"%s: %.2f %%\" % ('F1', f*100))\n",
    "    print('')\n",
    "\n",
    "print(\"Acc %.2f %% (+/- %.2f %%)\" % (np.mean(accuracy)*100, np.std(accuracy)*100))\n",
    "print(\"Precision %.2f %% (+/- %.2f %%)\" % (np.mean(precision)*100, np.std(precision)*100))\n",
    "print(\"Recall %.2f %% (+/- %.2f %%)\" % (np.mean(recall)*100, np.std(recall)*100))\n",
    "print(\"F1 %.2f %% (+/- %.2f %%)\" % (np.mean(f1)*100, np.std(f1)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
