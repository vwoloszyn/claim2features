{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def myHotEncode(input_data, max_vocab=0, vocab2idx=None):\n",
    "    \"Return the hot-vecotor and the vocab2idx.\"\n",
    "    if vocab2idx is None:\n",
    "        vocabFreq = {}\n",
    "        for i in input_data:\n",
    "            for j in i:\n",
    "                if j not in vocabFreq:\n",
    "                    vocabFreq[j] = 0\n",
    "                vocabFreq[j] += 1\n",
    "        vocabFreq = OrderedDict(sorted(vocabFreq.items(), key=itemgetter(1), reverse=True))\n",
    "        vocab2idx = {}\n",
    "        count = 0\n",
    "        for v in vocabFreq:\n",
    "            count += 1\n",
    "            if max_vocab > 0 and count > max_vocab:\n",
    "                break\n",
    "            vocab2idx[v] = len(vocab2idx)        \n",
    "    vocabEmbeddings = np.identity(len(vocab2idx), dtype='float32')\n",
    "    data_ret = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        for j in i:\n",
    "            if j in vocab2idx:\n",
    "                i_.append(vocabEmbeddings[vocab2idx[j]])\n",
    "        if len(i_) == 0:\n",
    "            i_ = np.zeros((1,len(vocab2idx)))\n",
    "        data_ret.append(sum(i_))\n",
    "    return data_ret, vocab2idx\n",
    "\n",
    "\n",
    "def myHotDecode(input_data, vocab2idx):\n",
    "    \"Return the decode as final representation and decode as indexs\"\n",
    "    data_ = []\n",
    "    data_idx = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        i_idx = []\n",
    "        if len(i) != len(vocab2idx):\n",
    "            print('Erro:', 'The vocab2idx not fit the input data!')\n",
    "            return\n",
    "        for _i, j in enumerate(i):\n",
    "            if j > 0:\n",
    "                v = [k for k in vocab2idx if vocab2idx[k]==_i][0]\n",
    "                i_.append(v)\n",
    "                i_idx.append(_i)\n",
    "        data_.append(i_)\n",
    "        data_idx.append(i_idx)\n",
    "    return data_, data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treat a list of str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def treat_str_list(input_list):\n",
    "    ret = []\n",
    "    for s in input_list:\n",
    "        ret += [ss.lower().strip() for ss in re.split('[\\W_]+', s)]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_path = 'features_annotated.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "columns = ['attrs', 'label', 'level', 'tag', 'text', 'url']\n",
    "\n",
    "last_url = ''\n",
    "#batches = []\n",
    "\n",
    "css_tss = []\n",
    "id_tss = []\n",
    "level_tss = []\n",
    "tag_tss = []\n",
    "url_tss = []\n",
    "css_ts = []\n",
    "id_ts = []\n",
    "level_ts = []\n",
    "tag_ts = []\n",
    "url_ts = []\n",
    "\n",
    "labels = []\n",
    "#labels_batch = []\n",
    "label = 'None'\n",
    "\n",
    "last_level = -1\n",
    "\n",
    "for attr, label_, level, tag, text, url in df[columns].values:\n",
    "    attr = ast.literal_eval(attr)\n",
    "    if url != last_url:\n",
    "        css_tss.append(css_ts.copy())\n",
    "        id_tss.append(id_ts.copy())\n",
    "        level_tss.append(level_ts.copy())\n",
    "        tag_tss.append(tag_ts.copy())\n",
    "        url_tss.append(last_url)\n",
    "        css_ts = []\n",
    "        id_ts = []\n",
    "        level_ts = []\n",
    "        tag_ts = []\n",
    "        #labels_batch.append(label)\n",
    "        #aux_list = []\n",
    "        #aux_list.append(css_tss.copy())\n",
    "        #aux_list.append(id_tss.copy())\n",
    "        #aux_list.append(level_tss.copy())\n",
    "        #aux_list.append(tag_tss.copy())\n",
    "        #batches.append(aux_list)\n",
    "        #css_tss = []\n",
    "        #id_tss = []\n",
    "        #level_tss = []\n",
    "        #tag_tss = []\n",
    "        labels.append(label) # new\n",
    "        #labels.append(labels_batch.copy())\n",
    "        #labels_batch = []\n",
    "        last_url = url\n",
    "        label = 'None'\n",
    "    if not level > last_level:\n",
    "        css_tss.append(css_ts.copy())\n",
    "        id_tss.append(id_ts.copy())\n",
    "        level_tss.append(level_ts.copy())\n",
    "        tag_tss.append(tag_ts.copy())\n",
    "        url_tss.append(last_url)\n",
    "        css_ts = css_ts[:level]\n",
    "        id_ts = id_ts[:level]\n",
    "        level_ts = level_ts[:level]\n",
    "        tag_ts = tag_ts[:level]\n",
    "        #labels_batch.append(label)\n",
    "        labels.append(label) # new\n",
    "        label = 'None'\n",
    "    if label_ != 'None':\n",
    "        label = label_\n",
    "    # Features\n",
    "    if 'class' in attr:\n",
    "        css_ts.append(treat_str_list(attr['class']))\n",
    "        #css_ts += treat_str_list(attr['class'])\n",
    "    else:\n",
    "        css_ts.append([])\n",
    "    aux_list = []\n",
    "    if 'id' in attr:\n",
    "        #aux_list.append(treat_str_list([attr['id']]))\n",
    "        aux_list += treat_str_list([attr['id']])\n",
    "    if 'name' in attr:\n",
    "        #aux_list.append(treat_str_list([attr['name']]))\n",
    "        aux_list += treat_str_list([attr['name']])\n",
    "        \n",
    "    id_ts.append(aux_list)\n",
    "    tag_ts.append([tag])\n",
    "    level_ts.append([level])\n",
    "    last_level = level\n",
    "\n",
    "css_tss.append(css_ts.copy())\n",
    "id_tss.append(id_ts.copy())\n",
    "level_tss.append(level_ts.copy())\n",
    "tag_tss.append(tag_ts.copy())\n",
    "url_tss.append(last_url)\n",
    "#labels_batch.append(label)\n",
    "#aux_list = []\n",
    "#aux_list.append(css_tss.copy())\n",
    "#aux_list.append(id_tss.copy())\n",
    "#aux_list.append(level_tss.copy())\n",
    "#aux_list.append(tag_tss.copy())\n",
    "#batches.append(aux_list)\n",
    "#labels.append(labels_batch.copy())\n",
    "labels.append(label) # new\n",
    "#batches = batches[1:]\n",
    "css_tss = css_tss[1:] # new\n",
    "id_tss = id_tss[1:] # new\n",
    "level_tss = level_tss[1:] # new\n",
    "tag_tss = tag_tss[1:] # new\n",
    "url_tss = url_tss[1:] # new\n",
    "labels = labels[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_ = []\n",
    "id_ = []\n",
    "tag_ = []\n",
    "for css in css_tss:\n",
    "    for i in css:\n",
    "        css_.append(i)\n",
    "for _id in id_tss:\n",
    "    for i in _id:\n",
    "        id_.append(i)\n",
    "for tag in tag_tss:\n",
    "    for i in tag:\n",
    "        tag_.append(i)\n",
    "_, css2Idx = myHotEncode(css_, 100)\n",
    "_, id2Idx = myHotEncode(id_, 100)\n",
    "_, tag2Idx = myHotEncode(tag_, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i0, batch in enumerate(batches):\n",
    "# CSS classes\n",
    "for i1, sample in enumerate(css_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=css2Idx)\n",
    "    css_tss[i1] = sample_\n",
    "# ID and Names\n",
    "for i1, sample in enumerate(id_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=id2Idx)\n",
    "    id_tss[i1] = sample_\n",
    "# Tags\n",
    "for i1, sample in enumerate(tag_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=tag2Idx)\n",
    "    tag_tss[i1] = sample_\n",
    "css_tss = np.array(css_tss)\n",
    "id_tss = np.array(id_tss)\n",
    "level_tss = np.array(level_tss)\n",
    "tag_tss = np.array(tag_tss)\n",
    "url_tss = np.array(url_tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "css_tss = pad_sequences(css_tss)\n",
    "id_tss = pad_sequences(id_tss)\n",
    "level_tss = pad_sequences(level_tss)\n",
    "tag_tss = pad_sequences(tag_tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, label2Idx = myHotEncode([[l] for l in labels])\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate,Concatenate\n",
    "\n",
    "\n",
    "def lstm_model():\n",
    "    input_css = Input(shape=(None, len(css2Idx)), name='input_css')\n",
    "    css = LSTM(len(css2Idx), return_sequences=True, name='lstm_css')(input_css)\n",
    "    \n",
    "    input_id = Input(shape=(None, len(id2Idx)), name='input_id')\n",
    "    _id = LSTM(len(id2Idx), return_sequences=True, name='lstm_id')(input_id)\n",
    "    \n",
    "    input_level = Input(shape=(None, 1), name='input_level')\n",
    "    level = LSTM(1, return_sequences=True, name='lstm_level')(input_level)\n",
    "    \n",
    "    input_tag = Input(shape=(None, len(tag2Idx)), name='input_tag')\n",
    "    tag = LSTM(len(tag2Idx), return_sequences=True, name='lstm_tag')(input_tag)\n",
    "    \n",
    "    output = concatenate([css, _id, level, tag], name='concat_inputs')\n",
    "    output = LSTM(256, name='lstm_concat')(output)\n",
    "    output = Dense(len(label2Idx), activation='softmax', name='dense_output')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_css, input_id, input_level, input_tag], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam')\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def lstm_model_old():\n",
    "    input_css = Input(shape=(None, len(css2Idx)), name='input_css')\n",
    "    css = LSTM(len(css2Idx), name='lstm_css')(input_css)\n",
    "    \n",
    "    input_id = Input(shape=(None, len(id2Idx)), name='input_id')\n",
    "    _id = LSTM(len(id2Idx), name='lstm_id')(input_id)\n",
    "    \n",
    "    input_level = Input(shape=(None, 1), name='input_level')\n",
    "    level = LSTM(1, name='lstm_level')(input_level)\n",
    "    \n",
    "    input_tag = Input(shape=(None, len(tag2Idx)), name='input_tag')\n",
    "    tag = LSTM(len(tag2Idx), name='lstm_tag')(input_tag)\n",
    "    \n",
    "    output = concatenate([css, _id, level, tag], name='concat_inputs')\n",
    "    output = Dense(256, name='dense_concat')(output)\n",
    "    output = Dense(len(label2Idx), activation='softmax', name='dense_output')(output)\n",
    "    model = Model(inputs=[input_css, input_id, input_level, input_tag], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam')\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Groups (URLs) {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "Test Groups (URLs) {0}\n",
      "Epoch 1/10\n",
      "2577/2577 [==============================] - 16s 6ms/step - loss: 0.4993\n",
      "Epoch 2/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.2554\n",
      "Epoch 3/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.2036\n",
      "Epoch 4/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.1562\n",
      "Epoch 5/10\n",
      "2577/2577 [==============================] - 10s 4ms/step - loss: 0.1376\n",
      "Epoch 6/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.1232\n",
      "Epoch 7/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.1133\n",
      "Epoch 8/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.1052\n",
      "Epoch 9/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.1027\n",
      "Epoch 10/10\n",
      "2577/2577 [==============================] - 11s 4ms/step - loss: 0.0967\n",
      "Acc: 73.93 %\n",
      "Precision: 28.05 %\n",
      "Recall: 25.13 %\n",
      "F1: 26.42 %\n",
      "\n",
      "Train Groups (URLs) {0, 2, 3, 4, 5, 6, 8, 9, 10}\n",
      "Test Groups (URLs) {1, 7}\n",
      "Epoch 1/10\n",
      "2704/2704 [==============================] - 14s 5ms/step - loss: 0.5472\n",
      "Epoch 2/10\n",
      "2704/2704 [==============================] - 11s 4ms/step - loss: 0.3017\n",
      "Epoch 3/10\n",
      "2704/2704 [==============================] - 12s 4ms/step - loss: 0.2385\n",
      "Epoch 4/10\n",
      "2704/2704 [==============================] - 11s 4ms/step - loss: 0.2020\n",
      "Epoch 5/10\n",
      "2704/2704 [==============================] - 11s 4ms/step - loss: 0.1727\n",
      "Epoch 6/10\n",
      "2704/2704 [==============================] - 12s 4ms/step - loss: 0.1536\n",
      "Epoch 7/10\n",
      "2704/2704 [==============================] - 11s 4ms/step - loss: 0.1503\n",
      "Epoch 8/10\n",
      "2704/2704 [==============================] - 11s 4ms/step - loss: 0.1353\n",
      "Epoch 9/10\n",
      "2704/2704 [==============================] - 11s 4ms/step - loss: 0.1240\n",
      "Epoch 10/10\n",
      "2704/2704 [==============================] - 11s 4ms/step - loss: 0.1221\n",
      "Acc: 83.33 %\n",
      "Precision: 30.07 %\n",
      "Recall: 46.62 %\n",
      "F1: 33.73 %\n",
      "\n",
      "Train Groups (URLs) {0, 1, 3, 4, 5, 6, 7, 10}\n",
      "Test Groups (URLs) {8, 9, 2}\n",
      "Epoch 1/10\n",
      "2569/2569 [==============================] - 14s 5ms/step - loss: 0.5496\n",
      "Epoch 2/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.2971\n",
      "Epoch 3/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.2390\n",
      "Epoch 4/10\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 0.1905\n",
      "Epoch 5/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.1684\n",
      "Epoch 6/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.1544\n",
      "Epoch 7/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.1349\n",
      "Epoch 8/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.1321\n",
      "Epoch 9/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.1238\n",
      "Epoch 10/10\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.1212\n",
      "Acc: 91.50 %\n",
      "Precision: 30.07 %\n",
      "Recall: 36.92 %\n",
      "F1: 32.37 %\n",
      "\n",
      "Train Groups (URLs) {0, 1, 2, 4, 5, 7, 8, 9, 10}\n",
      "Test Groups (URLs) {3, 6}\n",
      "Epoch 1/10\n",
      "2730/2730 [==============================] - 14s 5ms/step - loss: 0.5288\n",
      "Epoch 2/10\n",
      "2730/2730 [==============================] - 11s 4ms/step - loss: 0.2985\n",
      "Epoch 3/10\n",
      "2730/2730 [==============================] - 11s 4ms/step - loss: 0.2328\n",
      "Epoch 4/10\n",
      "2730/2730 [==============================] - 12s 4ms/step - loss: 0.1970\n",
      "Epoch 5/10\n",
      "2730/2730 [==============================] - 12s 4ms/step - loss: 0.1712\n",
      "Epoch 6/10\n",
      "2730/2730 [==============================] - 12s 5ms/step - loss: 0.1564\n",
      "Epoch 7/10\n",
      "2730/2730 [==============================] - 12s 4ms/step - loss: 0.1389\n",
      "Epoch 8/10\n",
      "2730/2730 [==============================] - 11s 4ms/step - loss: 0.1340\n",
      "Epoch 9/10\n",
      "2730/2730 [==============================] - 12s 4ms/step - loss: 0.1228\n",
      "Epoch 10/10\n",
      "2730/2730 [==============================] - 11s 4ms/step - loss: 0.1166\n",
      "Acc: 91.02 %\n",
      "Precision: 47.13 %\n",
      "Recall: 66.16 %\n",
      "F1: 54.04 %\n",
      "\n",
      "Train Groups (URLs) {0, 1, 2, 3, 6, 7, 8, 9}\n",
      "Test Groups (URLs) {10, 4, 5}\n",
      "Epoch 1/10\n",
      "2612/2612 [==============================] - 15s 6ms/step - loss: 0.5415\n",
      "Epoch 2/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.3127\n",
      "Epoch 3/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.2466\n",
      "Epoch 4/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.2189\n",
      "Epoch 5/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.1721\n",
      "Epoch 6/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.1572\n",
      "Epoch 7/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.1395\n",
      "Epoch 8/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.1314\n",
      "Epoch 9/10\n",
      "2612/2612 [==============================] - 11s 4ms/step - loss: 0.1252\n",
      "Epoch 10/10\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.1141\n",
      "Acc: 89.80 %\n",
      "Precision: 46.48 %\n",
      "Recall: 50.07 %\n",
      "F1: 46.64 %\n",
      "\n",
      "Acc 85.91 % (+/- 6.67 %)\n",
      "Precision 36.36 % (+/- 8.56 %)\n",
      "Recall 44.98 % (+/- 13.68 %)\n",
      "F1 38.64 % (+/- 10.14 %)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "confusion = []\n",
    "\n",
    "# Aux print\n",
    "_, url2Idx = myHotEncode([[u] for u in url_tss])\n",
    "\n",
    "gp = GroupKFold(n_splits=5)\n",
    "for train_indexs, test_indexs in gp.split(tag_tss, groups=url_tss):\n",
    "    \n",
    "    css_train = css_tss[train_indexs]\n",
    "    id_train = id_tss[train_indexs]\n",
    "    level_train = level_tss[train_indexs]\n",
    "    tag_train = tag_tss[train_indexs]\n",
    "    \n",
    "    X_train = [css_train, id_train, level_train, tag_train]\n",
    "    y_train = labels[train_indexs]\n",
    "    \n",
    "    css_test = css_tss[test_indexs]\n",
    "    id_test = id_tss[test_indexs]\n",
    "    level_test = level_tss[test_indexs]\n",
    "    tag_test = tag_tss[test_indexs]\n",
    "    \n",
    "    X_test = [css_test, id_test, level_test, tag_test]\n",
    "    y_test = labels[test_indexs]\n",
    "\n",
    "    groups_train = url_tss[train_indexs]\n",
    "    groups_test = url_tss[test_indexs]\n",
    "    train_g = set()\n",
    "    test_g = set()\n",
    "    for g in groups_train:\n",
    "        train_g.add(url2Idx[g])\n",
    "    for g in groups_test:\n",
    "        test_g.add(url2Idx[g])\n",
    "    print('Train Groups (URLs)', train_g)\n",
    "    print('Test Groups (URLs)', test_g)\n",
    "    \n",
    "    model = KerasClassifier(build_fn=lstm_model)\n",
    "    model.fit(X_train, y_train, epochs=10, verbose=1)\n",
    "    #model.fit(tags_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "    result = model.predict(X_test)\n",
    "    #result = model.predict(tags_test)\n",
    "    y_test = [r.tolist().index(1) for r in y_test]\n",
    "    acc = accuracy_score(result, y_test)\n",
    "    accuracy.append(acc)\n",
    "    p = precision_score(result, y_test, average=\"macro\")\n",
    "    precision.append(p)\n",
    "    r = recall_score(result, y_test, average=\"macro\")\n",
    "    recall.append(r)\n",
    "    f = f1_score(result, y_test, average=\"macro\")\n",
    "    f1.append(f)\n",
    "    confusion.append(confusion_matrix(result, y_test))\n",
    "    \n",
    "    print(\"%s: %.2f %%\" % ('Acc', acc*100))\n",
    "    print(\"%s: %.2f %%\" % ('Precision', p*100))\n",
    "    print(\"%s: %.2f %%\" % ('Recall', r*100))\n",
    "    print(\"%s: %.2f %%\" % ('F1', f*100))\n",
    "    print('')\n",
    "\n",
    "print(\"Acc %.2f %% (+/- %.2f %%)\" % (np.mean(accuracy)*100, np.std(accuracy)*100))\n",
    "print(\"Precision %.2f %% (+/- %.2f %%)\" % (np.mean(precision)*100, np.std(precision)*100))\n",
    "print(\"Recall %.2f %% (+/- %.2f %%)\" % (np.mean(recall)*100, np.std(recall)*100))\n",
    "print(\"F1 %.2f %% (+/- %.2f %%)\" % (np.mean(f1)*100, np.std(f1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = []\n",
    "for c in confusion:\n",
    "    d = len(label2Idx) - len(c)\n",
    "    c = np.pad(c, (0,d), 'constant')\n",
    "    nc.append(c)\n",
    "confusion = nc\n",
    "cm = np.mean(confusion, axis=0)\n",
    "plot_confusion_matrix(cm, label2Idx.keys(), normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
