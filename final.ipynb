{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def myHotEncode(input_data, max_vocab=0, vocab2idx=None):\n",
    "    \"Return the hot-vecotor and the vocab2idx.\"\n",
    "    if vocab2idx is None:\n",
    "        vocabFreq = {}\n",
    "        for i in input_data:\n",
    "            for j in i:\n",
    "                if j not in vocabFreq:\n",
    "                    vocabFreq[j] = 0\n",
    "                vocabFreq[j] += 1\n",
    "        vocabFreq = OrderedDict(sorted(vocabFreq.items(), key=itemgetter(1), reverse=True))\n",
    "        vocab2idx = {}\n",
    "        count = 0\n",
    "        for v in vocabFreq:\n",
    "            count += 1\n",
    "            if max_vocab > 0 and count > max_vocab:\n",
    "                break\n",
    "            vocab2idx[v] = len(vocab2idx)        \n",
    "    vocabEmbeddings = np.identity(len(vocab2idx), dtype='float32')\n",
    "    data_ret = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        for j in i:\n",
    "            if j in vocab2idx:\n",
    "                i_.append(vocabEmbeddings[vocab2idx[j]])\n",
    "        if len(i_) == 0:\n",
    "            i_ = np.zeros((1,len(vocab2idx)))\n",
    "        data_ret.append(sum(i_))\n",
    "    return data_ret, vocab2idx\n",
    "\n",
    "\n",
    "def myHotDecode(input_data, vocab2idx):\n",
    "    \"Return the decode as final representation and decode as indexs\"\n",
    "    data_ = []\n",
    "    data_idx = []\n",
    "    for i in input_data:\n",
    "        i_ = []\n",
    "        i_idx = []\n",
    "        if len(i) != len(vocab2idx):\n",
    "            print('Erro:', 'The vocab2idx not fit the input data!')\n",
    "            return\n",
    "        for _i, j in enumerate(i):\n",
    "            if j > 0:\n",
    "                v = [k for k in vocab2idx if vocab2idx[k]==_i][0]\n",
    "                i_.append(v)\n",
    "                i_idx.append(_i)\n",
    "        data_.append(i_)\n",
    "        data_idx.append(i_idx)\n",
    "    return data_, data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treat a list of str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def treat_str_list(input_list):\n",
    "    ret = []\n",
    "    for s in input_list:\n",
    "        ret += [ss.lower().strip() for ss in re.split('[\\W_]+', s)]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#data_path = 'features_annotated.csv'\n",
    "data_path = 'dataset.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "columns = ['attrs', 'label', 'level', 'tag', 'text', 'brother_attrs', 'brother_tag', 'brother_text', 'url', 'site']\n",
    "\n",
    "last_url = ''\n",
    "last_site = ''\n",
    "\n",
    "css_tss = []\n",
    "id_tss = []\n",
    "level_tss = []\n",
    "tag_tss = []\n",
    "url_tss = []\n",
    "text_tss = []\n",
    "site_tss = []\n",
    "brother_css_tss = []\n",
    "brother_id_tss = []\n",
    "brother_tag_tss = []\n",
    "brother_text_tss = []\n",
    "css_ts = []\n",
    "id_ts = []\n",
    "level_ts = []\n",
    "tag_ts = []\n",
    "url_ts = []\n",
    "text_ts = []\n",
    "site_ts = []\n",
    "brother_css_ts = []\n",
    "brother_id_ts = []\n",
    "brother_tag_ts = []\n",
    "brother_text_ts = []\n",
    "\n",
    "labels = []\n",
    "label = 'None'\n",
    "\n",
    "last_level = -1\n",
    "line = 0\n",
    "\n",
    "for attr, label_, level, tag, text, b_attr, b_tag, b_text, url, site in df[columns].values:\n",
    "    attr = ast.literal_eval(attr)\n",
    "    b_attr = ast.literal_eval(b_attr)\n",
    "    if url != last_url:\n",
    "        css_tss.append(css_ts.copy())\n",
    "        id_tss.append(id_ts.copy())\n",
    "        level_tss.append(level_ts.copy())\n",
    "        tag_tss.append(tag_ts.copy())\n",
    "        text_tss.append(text_ts.copy())\n",
    "        url_tss.append(last_url)\n",
    "        site_tss.append(last_site)\n",
    "        brother_css_tss.append(brother_css_ts.copy())\n",
    "        brother_id_tss.append(brother_id_ts.copy())\n",
    "        brother_tag_tss.append(brother_tag_ts.copy())\n",
    "        brother_text_tss.append(brother_text_ts.copy())\n",
    "        css_ts = []\n",
    "        id_ts = []\n",
    "        level_ts = []\n",
    "        tag_ts = []\n",
    "        text_ts = []\n",
    "        brother_css_ts = []\n",
    "        brother_id_ts = []\n",
    "        brother_tag_ts = []\n",
    "        brother_text_ts = []\n",
    "        labels.append(label)\n",
    "        last_url = url\n",
    "        last_site = site\n",
    "        label = 'None'\n",
    "        line = 0\n",
    "    if not level > last_level:\n",
    "        css_tss.append(css_ts.copy())\n",
    "        id_tss.append(id_ts.copy())\n",
    "        level_tss.append(level_ts.copy())\n",
    "        tag_tss.append(tag_ts.copy())\n",
    "        text_tss.append(text_ts.copy())\n",
    "        url_tss.append(last_url)\n",
    "        site_tss.append(last_site)\n",
    "        brother_css_tss.append(brother_css_ts.copy())\n",
    "        brother_id_tss.append(brother_id_ts.copy())\n",
    "        brother_tag_tss.append(brother_tag_ts.copy())\n",
    "        brother_text_tss.append(brother_text_ts.copy())\n",
    "        css_ts = css_ts[:level]\n",
    "        id_ts = id_ts[:level]\n",
    "        level_ts = level_ts[:level]\n",
    "        tag_ts = tag_ts[:level]\n",
    "        text_ts = text_ts[:level]\n",
    "        brother_css_ts = brother_css_ts[:level]\n",
    "        brother_id_ts = brother_id_ts[:level]\n",
    "        brother_tag_ts = brother_tag_ts[:level]\n",
    "        brother_text_ts = brother_text_ts[:level]\n",
    "        labels.append(label)\n",
    "        label = 'None'\n",
    "    if label_ != 'None':\n",
    "        label = label_\n",
    "    # Features\n",
    "    if 'class' in attr:\n",
    "        css_ts.append(treat_str_list(attr['class']))\n",
    "        #css_ts += treat_str_list(attr['class'])\n",
    "    else:\n",
    "        css_ts.append([])\n",
    "    if 'class' in b_attr:\n",
    "        brother_css_ts.append(treat_str_list(b_attr['class']))\n",
    "        #css_ts += treat_str_list(attr['class'])\n",
    "    else:\n",
    "        brother_css_ts.append([])\n",
    "    aux_list = []\n",
    "    if 'id' in attr:\n",
    "        #aux_list.append(treat_str_list([attr['id']]))\n",
    "        aux_list += treat_str_list([attr['id']])\n",
    "    if 'name' in attr:\n",
    "        #aux_list.append(treat_str_list([attr['name']]))\n",
    "        aux_list += treat_str_list([attr['name']])\n",
    "    id_ts.append(aux_list)\n",
    "    aux_list = []\n",
    "    if 'id' in b_attr:\n",
    "        #aux_list.append(treat_str_list([attr['id']]))\n",
    "        aux_list += treat_str_list([b_attr['id']])\n",
    "    if 'name' in b_attr:\n",
    "        #aux_list.append(treat_str_list([attr['name']]))\n",
    "        aux_list += treat_str_list([b_attr['name']])\n",
    "    brother_id_ts.append(aux_list)\n",
    "    tag_ts.append([tag])\n",
    "    brother_tag_ts.append([b_tag])\n",
    "    level_ts.append([level, line])\n",
    "    if text != 'None Text' and type(text) == str:\n",
    "        # ToDo - append ( [len(text)] + TFIDF_vector  ). Perhaps in another code block\n",
    "        #text_ts.append([len(text)])\n",
    "        text_ts.append([len(text)] + [text])\n",
    "    else:\n",
    "        text_ts.append([0] + [''])\n",
    "        \n",
    "    if b_text != 'None Text' and type(b_text) == str:\n",
    "        brother_text_ts.append([len(b_text)] + [b_text])\n",
    "    else:\n",
    "        brother_text_ts.append([0] + [''])\n",
    "        \n",
    "    last_level = level\n",
    "    line += line\n",
    "    \n",
    "css_tss.append(css_ts.copy())\n",
    "id_tss.append(id_ts.copy())\n",
    "level_tss.append(level_ts.copy())\n",
    "tag_tss.append(tag_ts.copy())\n",
    "text_tss.append(text_ts.copy())\n",
    "url_tss.append(last_url)\n",
    "labels.append(label) # new\n",
    "site_tss.append(last_site)\n",
    "brother_css_tss.append(brother_css_ts.copy())\n",
    "brother_id_tss.append(brother_id_ts.copy())\n",
    "brother_tag_tss.append(brother_tag_ts.copy())\n",
    "brother_text_tss.append(brother_text_ts.copy())\n",
    "#batches = batches[1:]\n",
    "css_tss = css_tss[1:] # new\n",
    "id_tss = id_tss[1:] # new\n",
    "level_tss = level_tss[1:] # new\n",
    "tag_tss = tag_tss[1:] # new\n",
    "text_tss = text_tss[1:] # new\n",
    "url_tss = url_tss[1:] # new\n",
    "labels = labels[1:]\n",
    "brother_css_tss = brother_css_tss[1:]\n",
    "brother_id_tss = brother_id_tss[1:]\n",
    "brother_tag_tss = brother_tag_tss[1:]\n",
    "brother_text_tss = brother_text_tss[1:]\n",
    "site_tss = site_tss[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_ = []\n",
    "id_ = []\n",
    "tag_ = []\n",
    "text_ = []\n",
    "for css in css_tss:\n",
    "    for i in css:\n",
    "        css_.append(i)\n",
    "for _id in id_tss:\n",
    "    for i in _id:\n",
    "        id_.append(i)\n",
    "for tag in tag_tss:\n",
    "    for i in tag:\n",
    "        tag_.append(i)\n",
    "for text in text_tss:\n",
    "    for i in text:\n",
    "        if i[1] != '':\n",
    "            text_.append(i[1])\n",
    "_, css2Idx = myHotEncode(css_, 100)\n",
    "_, id2Idx = myHotEncode(id_, 100)\n",
    "_, tag2Idx = myHotEncode(tag_, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CSS classes\n",
    "for i1, sample in enumerate(css_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=css2Idx)\n",
    "    css_tss[i1] = sample_\n",
    "for i1, sample in enumerate(brother_css_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=css2Idx)\n",
    "    brother_css_tss[i1] = sample_\n",
    "# ID and Names\n",
    "for i1, sample in enumerate(id_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=id2Idx)\n",
    "    id_tss[i1] = sample_\n",
    "for i1, sample in enumerate(brother_id_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=id2Idx)\n",
    "    brother_id_tss[i1] = sample_\n",
    "# Tags\n",
    "for i1, sample in enumerate(tag_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=tag2Idx)\n",
    "    tag_tss[i1] = sample_\n",
    "for i1, sample in enumerate(brother_tag_tss):\n",
    "    sample_, _ = myHotEncode(sample, vocab2idx=tag2Idx)\n",
    "    brother_tag_tss[i1] = sample_\n",
    "css_tss = np.array(css_tss)\n",
    "id_tss = np.array(id_tss)\n",
    "level_tss = np.array(level_tss)\n",
    "tag_tss = np.array(tag_tss)\n",
    "text_tss = np.array(text_tss)\n",
    "url_tss = np.array(url_tss)\n",
    "brother_css_tss = np.array(brother_css_tss)\n",
    "brother_id_tss = np.array(brother_id_tss)\n",
    "brother_tag_tss = np.array(brother_tag_tss)\n",
    "brother_text_tss = np.array(brother_text_tss)\n",
    "site_tss = np.array(site_tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "vocab_size = 1000\n",
    "vec = TfidfVectorizer(max_features=vocab_size, stop_words=ENGLISH_STOP_WORDS)\n",
    "vec.fit(text_)\n",
    "\n",
    "for i, text in enumerate(text_tss):\n",
    "    for j, t in enumerate(text):\n",
    "        text_tss[i][j] = np.concatenate(([t[0]], vec.transform([t[1]]).toarray()[0]))\n",
    "for i, text in enumerate(brother_text_tss):\n",
    "    for j, t in enumerate(text):\n",
    "        brother_text_tss[i][j] = np.concatenate(([t[0]], vec.transform([t[1]]).toarray()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "css_tss = pad_sequences(css_tss, maxlen=10)\n",
    "id_tss = pad_sequences(id_tss, maxlen=10)\n",
    "level_tss = pad_sequences(level_tss, maxlen=10)\n",
    "tag_tss = pad_sequences(tag_tss, maxlen=10)\n",
    "text_tss = pad_sequences(text_tss, maxlen=10)\n",
    "\n",
    "brother_css_tss = pad_sequences(brother_css_tss, maxlen=10)\n",
    "brother_id_tss = pad_sequences(brother_id_tss, maxlen=10)\n",
    "brother_tag_tss = pad_sequences(brother_tag_tss, maxlen=10)\n",
    "brother_text_tss = pad_sequences(brother_text_tss, maxlen=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, label2Idx = myHotEncode([[l] for l in labels])\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate,Concatenate\n",
    "\n",
    "\n",
    "def lstm_model():\n",
    "    input_css = Input(shape=(None, len(css2Idx)), name='input_css')\n",
    "    css = LSTM(len(css2Idx), return_sequences=True, name='lstm_css')(input_css)\n",
    "    \n",
    "    input_brother_css = Input(shape=(None, len(css2Idx)), name='input_brother_css')\n",
    "    brother_css = LSTM(len(css2Idx), return_sequences=True, name='lstm_brother_css')(input_brother_css)\n",
    "    \n",
    "    input_id = Input(shape=(None, len(id2Idx)), name='input_id')\n",
    "    _id = LSTM(len(id2Idx), return_sequences=True, name='lstm_id')(input_id)\n",
    "    \n",
    "    input_brother_id = Input(shape=(None, len(id2Idx)), name='input_brother_id')\n",
    "    brother_id = LSTM(len(id2Idx), return_sequences=True, name='lstm_brother_id')(input_brother_id)\n",
    "    \n",
    "    input_level = Input(shape=(None, 2), name='input_level')\n",
    "    level = LSTM(4, return_sequences=True, name='lstm_level')(input_level)\n",
    "    \n",
    "    input_tag = Input(shape=(None, len(tag2Idx)), name='input_tag')\n",
    "    tag = LSTM(len(tag2Idx), return_sequences=True, name='lstm_tag')(input_tag)\n",
    "    \n",
    "    input_brother_tag = Input(shape=(None, len(tag2Idx)), name='input_brother_tag')\n",
    "    brother_tag = LSTM(len(tag2Idx), return_sequences=True, name='lstm_brother_tag')(input_brother_tag)\n",
    "    \n",
    "    input_text = Input(shape=(None, (1+vocab_size)), name='input_text')\n",
    "    text = LSTM((1+vocab_size), return_sequences=True, name='lstm_text')(input_text)\n",
    "    \n",
    "    input_brother_text = Input(shape=(None, (1+vocab_size)), name='input_brother_text')\n",
    "    brother_text = LSTM((1+vocab_size), return_sequences=True, name='lstm_brother_text')(input_brother_text)\n",
    "    \n",
    "    output = concatenate([css, brother_css, _id, brother_id, level, tag, brother_tag, text, brother_text], name='concat_inputs')\n",
    "    output = LSTM(256, name='lstm_concat')(output)\n",
    "    output = Dense(len(label2Idx), activation='softmax', name='dense_output')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_css, input_brother_css, input_id, input_brother_id, input_level, input_tag, input_brother_tag, input_text, input_brother_text], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam')\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def lstm_model_old():\n",
    "    input_css = Input(shape=(None, len(css2Idx)), name='input_css')\n",
    "    css = LSTM(len(css2Idx), name='lstm_css')(input_css)\n",
    "    \n",
    "    input_id = Input(shape=(None, len(id2Idx)), name='input_id')\n",
    "    _id = LSTM(len(id2Idx), name='lstm_id')(input_id)\n",
    "    \n",
    "    input_level = Input(shape=(None, 1), name='input_level')\n",
    "    level = LSTM(1, name='lstm_level')(input_level)\n",
    "    \n",
    "    input_tag = Input(shape=(None, len(tag2Idx)), name='input_tag')\n",
    "    tag = LSTM(len(tag2Idx), name='lstm_tag')(input_tag)\n",
    "    \n",
    "    output = concatenate([css, _id, level, tag], name='concat_inputs')\n",
    "    output = Dense(256, name='dense_concat')(output)\n",
    "    output = Dense(len(label2Idx), activation='softmax', name='dense_output')(output)\n",
    "    model = Model(inputs=[input_css, input_id, input_level, input_tag], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam')\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_tss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "confusion = []\n",
    "\n",
    "# Aux print\n",
    "_, site2Idx = myHotEncode([[u] for u in site_tss])\n",
    "\n",
    "gp = GroupKFold(n_splits=3)\n",
    "for train_indexs, test_indexs in gp.split(tag_tss, groups=site_tss):\n",
    "    \n",
    "    css_train = css_tss[train_indexs]\n",
    "    brother_css_train = brother_css_tss[train_indexs]\n",
    "    id_train = id_tss[train_indexs]\n",
    "    brother_id_train = brother_id_tss[train_indexs]\n",
    "    level_train = level_tss[train_indexs]\n",
    "    tag_train = tag_tss[train_indexs]\n",
    "    brother_tag_train = brother_tag_tss[train_indexs]\n",
    "    text_train = text_tss[train_indexs]\n",
    "    brother_text_train = brother_text_tss[train_indexs]\n",
    "    \n",
    "    X_train = [css_train, brother_css_train, id_train, brother_id_train, level_train, tag_train, brother_tag_train, text_train, brother_text_train]\n",
    "    y_train = labels[train_indexs]\n",
    "    \n",
    "    css_test = css_tss[test_indexs]\n",
    "    brother_css_test = brother_css_tss[test_indexs]\n",
    "    id_test = id_tss[test_indexs]\n",
    "    brother_id_test = brother_id_tss[test_indexs]\n",
    "    level_test = level_tss[test_indexs]\n",
    "    tag_test = tag_tss[test_indexs]\n",
    "    brother_tag_test = brother_tag_tss[test_indexs]\n",
    "    text_test = text_tss[test_indexs]\n",
    "    brother_text_test = brother_text_tss[test_indexs]\n",
    "    \n",
    "    X_test = [css_test, brother_css_test, id_test, brother_id_test, level_test, tag_test, brother_tag_test, text_test, brother_text_test]\n",
    "    y_test = labels[test_indexs]\n",
    "\n",
    "    groups_train = site_tss[train_indexs]\n",
    "    groups_test = site_tss[test_indexs]\n",
    "    train_g = set()\n",
    "    test_g = set()\n",
    "    for g in groups_train:\n",
    "        train_g.add(site2Idx[g])\n",
    "    for g in groups_test:\n",
    "        test_g.add(site2Idx[g])\n",
    "    print('Train Groups (URLs)', train_g)\n",
    "    print('Test Groups (URLs)', test_g)\n",
    "    \n",
    "    model = KerasClassifier(build_fn=lstm_model)\n",
    "    model.fit(X_train, y_train, epochs=10, verbose=1)\n",
    "    #model.fit(tags_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "    result = model.predict(X_test)\n",
    "    #result = model.predict(tags_test)\n",
    "    y_test = [r.tolist().index(1) for r in y_test]\n",
    "    acc = accuracy_score(result, y_test)\n",
    "    accuracy.append(acc)\n",
    "    p = precision_score(result, y_test, average=\"macro\")\n",
    "    precision.append(p)\n",
    "    r = recall_score(result, y_test, average=\"macro\")\n",
    "    recall.append(r)\n",
    "    f = f1_score(result, y_test, average=\"macro\")\n",
    "    f1.append(f)\n",
    "    confusion.append(confusion_matrix(result, y_test))\n",
    "    \n",
    "    print(\"%s: %.2f %%\" % ('Acc', acc*100))\n",
    "    print(\"%s: %.2f %%\" % ('Precision', p*100))\n",
    "    print(\"%s: %.2f %%\" % ('Recall', r*100))\n",
    "    print(\"%s: %.2f %%\" % ('F1', f*100))\n",
    "    print('')\n",
    "\n",
    "print(\"Acc %.2f %% (+/- %.2f %%)\" % (np.mean(accuracy)*100, np.std(accuracy)*100))\n",
    "print(\"Precision %.2f %% (+/- %.2f %%)\" % (np.mean(precision)*100, np.std(precision)*100))\n",
    "print(\"Recall %.2f %% (+/- %.2f %%)\" % (np.mean(recall)*100, np.std(recall)*100))\n",
    "print(\"F1 %.2f %% (+/- %.2f %%)\" % (np.mean(f1)*100, np.std(f1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nc = []\n",
    "for c in confusion:\n",
    "    d = len(label2Idx) - len(c)\n",
    "    c = np.pad(c, (0,d), 'constant')\n",
    "    nc.append(c)\n",
    "confusion = nc\n",
    "cm = np.mean(confusion, axis=0)\n",
    "plot_confusion_matrix(cm, label2Idx.keys(), normalize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
